# ============================================================
# GOOGLE COLAB - RAG ARCHITECTURE 1 (COMPLETE VERSION)
# ============================================================
# Reliability-focused RAG: DPR + ColBERT + FAISS + FLAN-T5
# "Yanlƒ±≈ü cevap vermektense hi√ß cevap verme" felsefesi
# ============================================================

# --- CELL 1: Install Dependencies ---
!pip install -q torch transformers faiss-gpu numpy pandas tqdm rouge-score sacrebleu requests psutil sentence-transformers

# --- CELL 2: Create Directory Structure ---
!mkdir -p scripts data indexes outputs/logs

# --- CELL 3: arch1_rerank.py (Cross-Encoder) ---
%%writefile scripts/arch1_rerank.py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from typing import List, Tuple
from tqdm import tqdm

class Reranker:
    def __init__(self, model_name="cross-encoder/ms-marco-MiniLM-L-6-v2", device="cuda", batch_size=64):
        self.device = device if torch.cuda.is_available() else "cpu"
        self.batch_size = batch_size
        print(f"Loading Reranker: {model_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(self.device)
        self.model.eval()
    
    def compute_scores(self, pairs: List[List[str]], show_progress=True) -> List[float]:
        scores = []
        iterator = tqdm(range(0, len(pairs), self.batch_size), desc="Reranking") if show_progress else range(0, len(pairs), self.batch_size)
        with torch.inference_mode():
            for i in iterator:
                batch = pairs[i:i+self.batch_size]
                inputs = self.tokenizer(batch, padding=True, truncation=True, return_tensors="pt", max_length=512).to(self.device)
                logits = self.model(**inputs).logits.squeeze(-1).cpu().tolist()
                if isinstance(logits, float): logits = [logits]
                scores.extend(logits)
        return scores
    
    def rerank(self, query: str, documents: List[str], top_k: int = None) -> List[Tuple[str, float]]:
        pairs = [[query, doc] for doc in documents]
        scores = self.compute_scores(pairs, show_progress=False)
        results = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
        return results[:top_k] if top_k else results

# --- CELL 4: arch1_colbert.py (Token-Level Reranking) ---
%%writefile scripts/arch1_colbert.py
import torch
import numpy as np
from typing import List, Tuple, Optional
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm

class ColBERTReranker:
    """ColBERT-style reranker using MaxSim (late interaction)."""
    
    def __init__(self, model_name="bert-base-uncased", device="cuda", dim=128, batch_size=32):
        self.device = device if torch.cuda.is_available() else "cpu"
        self.dim = dim
        self.batch_size = batch_size
        
        print(f"üî∑ Loading ColBERT Reranker on {self.device}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(self.device)
        self.model.eval()
        
        hidden_size = self.model.config.hidden_size
        self.projection = torch.nn.Linear(hidden_size, dim).to(self.device)
        torch.nn.init.xavier_uniform_(self.projection.weight)
        print("‚úÖ ColBERT ready!")
    
    def _encode_tokens(self, texts, is_query=False):
        prefix = "[Q] " if is_query else "[D] "
        texts = [prefix + t for t in texts]
        inputs = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=512).to(self.device)
        with torch.no_grad():
            outputs = self.model(**inputs)
            projected = self.projection(outputs.last_hidden_state)
            projected = torch.nn.functional.normalize(projected, p=2, dim=-1)
        return projected, inputs.attention_mask
    
    def _maxsim(self, q_emb, q_mask, d_emb, d_mask):
        similarity = torch.matmul(q_emb.squeeze(0), d_emb.squeeze(0).T)
        d_mask_exp = d_mask.squeeze(0).unsqueeze(0).expand_as(similarity)
        similarity = similarity.masked_fill(~d_mask_exp.bool(), float('-inf'))
        max_sim = similarity.max(dim=1).values
        return max_sim[q_mask.squeeze(0).bool()].sum().item()
    
    def compute_scores(self, query, documents, show_progress=True):
        q_emb, q_mask = self._encode_tokens([query], is_query=True)
        scores = []
        iterator = tqdm(range(0, len(documents), self.batch_size), desc="ColBERT") if show_progress else range(0, len(documents), self.batch_size)
        for i in iterator:
            batch = documents[i:i+self.batch_size]
            d_embs, d_masks = self._encode_tokens(batch, is_query=False)
            for j in range(len(batch)):
                scores.append(self._maxsim(q_emb, q_mask, d_embs[j:j+1], d_masks[j:j+1]))
        return scores
    
    def rerank(self, query, documents, top_k=None):
        scores = self.compute_scores(query, documents, show_progress=False)
        results = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
        return results[:top_k] if top_k else results

# --- CELL 5: arch1_retriever.py (Unified Retriever) ---
%%writefile scripts/arch1_retriever.py
import torch
import faiss
import numpy as np
import json
import os
from dataclasses import dataclass, field
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer
import arch1_rerank
import arch1_colbert

@dataclass
class RetrievalResult:
    doc_id: int
    chunk_id: int
    text: str
    dpr_score: float
    rerank_score: float
    metadata: Dict = field(default_factory=dict)

@dataclass 
class RetrievalResponse:
    query: str
    dpr_results: List[RetrievalResult]
    reranked_results: List[RetrievalResult]
    selected_contexts: List[str]
    total_context_tokens: int
    hit_at_k: Optional[int] = None

class RAGRetriever:
    def __init__(self, passages_path="indexes/passages.txt", metadata_path="indexes/passages_metadata.jsonl",
                 index_path="indexes/nq_hnsw.index", device="cuda", reranker_type="colbert"):
        self.device = device if torch.cuda.is_available() else "cpu"
        self.max_context_tokens = 2048
        self.reranker_type = reranker_type
        
        print(f"üîç Initializing RAG Retriever on {self.device}...")
        
        with open(passages_path, 'r') as f:
            self.passages = [line.strip() for line in f if line.strip()]
        print(f"   ‚úÖ Loaded {len(self.passages)} passages")
        
        self.metadata = {}
        if os.path.exists(metadata_path):
            with open(metadata_path, 'r') as f:
                for i, line in enumerate(f):
                    try: self.metadata[i] = json.loads(line.strip())
                    except: pass
        
        print("   Loading DPR Question Encoder...")
        self.q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
        self.q_model = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base").to(self.device)
        self.q_model.eval()
        
        print("   Loading FAISS index...")
        self.index = faiss.read_index(index_path)
        if hasattr(self.index, 'hnsw'): self.index.hnsw.efSearch = 128
        
        if reranker_type == "colbert":
            print("   Loading ColBERT Reranker (token-level)...")
            self.reranker = arch1_colbert.ColBERTReranker(device=self.device)
        else:
            print("   Loading Cross-Encoder Reranker...")
            self.reranker = arch1_rerank.Reranker(device=self.device)
        
        print(f"‚úÖ Retriever ready! (reranker: {reranker_type})\\n")
    
    def _apply_recency_boost(self, results, boost_hours=24, boost_factor=0.15):
        now = datetime.now()
        cutoff = now - timedelta(hours=boost_hours)
        for r in results:
            ts = r.metadata.get("ingested_at", "")
            if ts:
                try:
                    if datetime.fromisoformat(ts) > cutoff:
                        r.rerank_score *= (1 + boost_factor)
                        r.metadata["recency_boosted"] = True
                except: pass
        return results
    
    def retrieve(self, query, top_n_coarse=100, top_k_rerank=10, score_threshold=0.0, 
                 gold_passage_id=None, apply_recency_boost=False):
        with torch.no_grad():
            inputs = self.q_tokenizer(query, return_tensors="pt", max_length=256, truncation=True).to(self.device)
            q_emb = self.q_model(**inputs).pooler_output.cpu().numpy().astype('float32')
        
        distances, indices = self.index.search(q_emb, top_n_coarse)
        
        dpr_results = []
        for dist, idx in zip(distances[0], indices[0]):
            if 0 <= idx < len(self.passages):
                dpr_results.append(RetrievalResult(doc_id=idx, chunk_id=len(dpr_results), text=self.passages[idx],
                                                   dpr_score=float(dist), rerank_score=0.0, metadata=self.metadata.get(idx, {})))
        
        if dpr_results:
            if self.reranker_type == "colbert":
                scores = self.reranker.compute_scores(query, [r.text for r in dpr_results], show_progress=False)
            else:
                pairs = [[query, r.text] for r in dpr_results]
                scores = self.reranker.compute_scores(pairs, show_progress=False)
            for r, s in zip(dpr_results, scores): r.rerank_score = s
            reranked = sorted(dpr_results, key=lambda x: x.rerank_score, reverse=True)
        else:
            reranked = []
        
        if apply_recency_boost:
            reranked = self._apply_recency_boost(reranked)
            reranked = sorted(reranked, key=lambda x: x.rerank_score, reverse=True)
        
        filtered = [r for r in reranked if r.rerank_score >= score_threshold][:top_k_rerank]
        contexts = [r.text for r in filtered]
        tokens = sum(int(len(c.split()) * 1.3) for c in contexts)
        
        hit = None
        if gold_passage_id is not None:
            for i, r in enumerate(reranked):
                if r.doc_id == gold_passage_id: hit = i + 1; break
        
        return RetrievalResponse(query=query, dpr_results=dpr_results[:10], reranked_results=filtered,
                                  selected_contexts=contexts, total_context_tokens=tokens, hit_at_k=hit)

# --- CELL 6: arch1_generate.py (Faithfulness-Oriented) ---
%%writefile scripts/arch1_generate.py
import torch
import re
from dataclasses import dataclass
from typing import List, Optional
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

@dataclass
class GenerationResult:
    query: str
    answer: str
    citations: List[int]
    confidence: float
    is_grounded: bool
    is_no_answer: bool
    context_used: str
    reasoning: str

class RAGGenerator:
    def __init__(self, model_name="google/flan-t5-base", device="cuda", no_answer_threshold=0.3):
        self.device = device if torch.cuda.is_available() else "cpu"
        self.no_answer_threshold = no_answer_threshold
        print(f"ü§ñ Loading RAG Generator on {self.device}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)
        self.model.eval()
        print("‚úÖ Generator ready!")
    
    def _compute_grounding(self, answer, contexts):
        if not answer or not contexts: return 0.0, False
        answer_lower = answer.lower()
        no_answer_phrases = ["bilmiyorum", "i don't know", "yeterli bilgi yok"]
        if any(p in answer_lower for p in no_answer_phrases): return 1.0, True
        
        answer_words = set(re.findall(r'\\w+', answer_lower))
        if not answer_words: return 0.0, False
        
        all_words = set()
        for c in contexts: all_words.update(re.findall(r'\\w+', c.lower()))
        
        overlap = len(answer_words & all_words) / len(answer_words)
        return overlap, overlap >= 0.2
    
    def generate(self, query, contexts, doc_ids, max_length=150, require_grounding=True):
        if not contexts:
            return GenerationResult(query=query, answer="Bilmiyorum - context yok.", citations=[],
                                    confidence=0.0, is_grounded=False, is_no_answer=True, context_used="", reasoning="No context")
        
        ctx_str = "\\n".join([f"[Source {i+1}]: {c}" for i, c in enumerate(contexts)])
        prompt = f"Answer based ONLY on these sources. If unsure, say 'Bilmiyorum'.\\n\\n{ctx_str}\\n\\nQuestion: {query}\\nAnswer:"
        
        inputs = self.tokenizer(prompt, return_tensors="pt", max_length=1024, truncation=True).to(self.device)
        with torch.no_grad():
            outputs = self.model.generate(**inputs, max_length=max_length, num_beams=4, early_stopping=True)
        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
        
        confidence, is_grounded = self._compute_grounding(answer, contexts)
        is_no_answer = any(p in answer.lower() for p in ["bilmiyorum", "don't know"])
        
        if require_grounding and not is_grounded and confidence < self.no_answer_threshold:
            answer = "Bilmiyorum - kaynaklarda yeterli bilgi yok."
            is_no_answer = True
        
        return GenerationResult(query=query, answer=answer, citations=doc_ids[:3], confidence=confidence,
                                is_grounded=is_grounded, is_no_answer=is_no_answer, context_used=ctx_str[:300],
                                reasoning=f"Confidence={confidence:.2f}")

# --- CELL 7: arch1_pipeline.py (End-to-End RAG) ---
%%writefile scripts/arch1_pipeline.py
import json
import os
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import List, Optional
from arch1_retriever import RAGRetriever
from arch1_generate import RAGGenerator

@dataclass
class RAGResponse:
    query: str
    answer: str
    citations: List[int]
    confidence: float
    is_grounded: bool
    is_no_answer: bool
    retrieval_contexts: int
    hit_at_k: Optional[int]
    timestamp: str

class RAGPipeline:
    def __init__(self, passages_path="indexes/passages.txt", index_path="indexes/nq_hnsw.index",
                 device="cuda", llm_model="google/flan-t5-base", reranker_type="colbert"):
        print("=" * 60)
        print("üöÄ INITIALIZING RAG PIPELINE (Architecture 1)")
        print("=" * 60)
        self.retriever = RAGRetriever(passages_path=passages_path, index_path=index_path, 
                                       device=device, reranker_type=reranker_type)
        self.generator = RAGGenerator(model_name=llm_model, device=device)
        os.makedirs("outputs/logs", exist_ok=True)
        print("\\n‚úÖ RAG PIPELINE READY\\n")
    
    def query(self, question, top_k=10, gold_passage_id=None, apply_recency_boost=False, verbose=True):
        if verbose: print(f"\\nüì© Query: {question}")
        
        retrieval = self.retriever.retrieve(query=question, top_k_rerank=top_k, 
                                            gold_passage_id=gold_passage_id, apply_recency_boost=apply_recency_boost)
        doc_ids = [r.doc_id for r in retrieval.reranked_results]
        
        generation = self.generator.generate(query=question, contexts=retrieval.selected_contexts, doc_ids=doc_ids)
        
        if verbose:
            print(f"\\nü§ñ Answer: {generation.answer}")
            print(f"üìö Citations: {generation.citations}")
            print(f"üéØ Grounded: {'‚úÖ' if generation.is_grounded else '‚ùå'}")
            if generation.is_no_answer: print("‚ö†Ô∏è No-answer triggered")
        
        return RAGResponse(query=question, answer=generation.answer, citations=generation.citations,
                           confidence=generation.confidence, is_grounded=generation.is_grounded,
                           is_no_answer=generation.is_no_answer, retrieval_contexts=len(retrieval.selected_contexts),
                           hit_at_k=retrieval.hit_at_k, timestamp=datetime.now().isoformat())

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--query", "-q", type=str, help="Query to run")
    parser.add_argument("--interactive", "-i", action="store_true")
    args = parser.parse_args()
    
    pipeline = RAGPipeline()
    
    if args.query:
        pipeline.query(args.query)
    elif args.interactive:
        print("üéÆ Interactive Mode - Type 'exit' to quit")
        while True:
            q = input("\\n‚ùì Question: ").strip()
            if q.lower() in ['exit', 'quit', 'q']: break
            if q: pipeline.query(q)

# --- CELL 8: arch1_prepare_data.py (Data with Metadata) ---
%%writefile scripts/arch1_prepare_data.py
import requests
import random
import json
import os
from datetime import datetime
from typing import List, Dict

CONFIG = {"URL": "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt", "CHUNK_SIZE": 100, "OVERLAP": 30, "SEED": 42}
random.seed(CONFIG["SEED"])

def download_data():
    print("Downloading Tiny Shakespeare...")
    return requests.get(CONFIG["URL"]).text

def chunk_text(text, chunk_size=100, overlap=30):
    words = text.split()
    chunks = []
    step = max(1, chunk_size - overlap)
    for i in range(0, len(words), step):
        chunk = " ".join(words[i:i+chunk_size])
        if chunk.strip(): chunks.append(chunk)
        if i + chunk_size >= len(words): break
    return chunks

def generate_qa(passages, n):
    qa = []
    indices = list(range(len(passages)))
    random.shuffle(indices)
    for idx in indices[:n]:
        sentences = [s.strip() for s in passages[idx].replace("?", ".").replace("!", ".").split(".") if len(s.strip().split()) > 8]
        if sentences:
            q = random.choice(sentences)
            qa.append({"id": str(len(qa)), "question": q, "answers": [q], "gold_passage_id": str(idx), "gold_passage_text": passages[idx]})
    return qa

def main():
    os.makedirs("indexes", exist_ok=True)
    os.makedirs("data", exist_ok=True)
    
    text = download_data()
    passages = chunk_text(text, CONFIG["CHUNK_SIZE"], CONFIG["OVERLAP"])
    print(f"‚úÖ Created {len(passages)} passages (with {CONFIG['OVERLAP']} word overlap)")
    
    with open("indexes/passages.txt", "w") as f:
        for p in passages: f.write(p + "\\n")
    
    with open("indexes/passages_metadata.jsonl", "w") as f:
        for i, p in enumerate(passages):
            meta = {"doc_id": str(i), "chunk_id": i, "source": "shakespeare", "ingested_at": datetime.now().isoformat(), "section": f"Part {i//50+1}", "doc_version": "1.0"}
            f.write(json.dumps(meta) + "\\n")
    
    qa = generate_qa(passages, min(200, len(passages)))
    with open("data/eval_qa.jsonl", "w") as f:
        for item in qa: f.write(json.dumps(item) + "\\n")
    
    print(f"‚úÖ Saved {len(qa)} QA pairs")

if __name__ == "__main__":
    main()

# --- CELL 9: arch1_embeddings.py ---
%%writefile scripts/arch1_embeddings.py
import torch
import numpy as np
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from tqdm import tqdm

def encode_passages(passages_path="indexes/passages.txt", output_path="indexes/passage_emb.npy", batch_size=64, device="cuda"):
    device = device if torch.cuda.is_available() else "cpu"
    with open(passages_path, 'r') as f:
        passages = [l.strip() for l in f if l.strip()]
    
    print(f"Encoding {len(passages)} passages with DPR...")
    tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
    model = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base").to(device).eval()
    
    embeddings = []
    with torch.no_grad():
        for i in tqdm(range(0, len(passages), batch_size)):
            batch = passages[i:i+batch_size]
            inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=256).to(device)
            embeddings.append(model(**inputs).pooler_output.cpu().numpy())
    
    np.save(output_path, np.concatenate(embeddings, 0))
    print(f"‚úÖ Saved embeddings to {output_path}")

if __name__ == "__main__":
    encode_passages()

# --- CELL 10: arch1_faiss.py (with Incremental Support) ---
%%writefile scripts/arch1_faiss.py
import faiss
import numpy as np
import os
import json

CONFIG = {"M": 64, "EF_CONSTRUCTION": 200}

def build_index(embedding_path="indexes/passage_emb.npy", index_path="indexes/nq_hnsw.index"):
    print("Building FAISS HNSW index...")
    embeddings = np.load(embedding_path).astype('float32')
    index = faiss.IndexHNSWFlat(embeddings.shape[1], CONFIG["M"])
    index.hnsw.efConstruction = CONFIG["EF_CONSTRUCTION"]
    index.add(embeddings)
    faiss.write_index(index, index_path)
    print(f"‚úÖ Index saved ({index.ntotal} vectors)")

def add_documents(index_path, new_embeddings, id_map_path="indexes/id_map.json"):
    """Incremental: Add new documents to existing index."""
    from datetime import datetime
    index = faiss.read_index(index_path)
    current = index.ntotal
    index.add(new_embeddings.astype('float32'))
    faiss.write_index(index, index_path)
    
    id_map = json.load(open(id_map_path)) if os.path.exists(id_map_path) else {}
    for i in range(len(new_embeddings)):
        id_map[str(current + i)] = {"added_at": datetime.now().isoformat()}
    json.dump(id_map, open(id_map_path, 'w'), indent=2)
    
    print(f"‚úÖ Added {len(new_embeddings)} docs. Total: {index.ntotal}")
    return len(new_embeddings)

if __name__ == "__main__":
    build_index()

# --- CELL 11: Run Full Pipeline ---
print("="*60)
print("üöÄ RUNNING FULL RAG PIPELINE")
print("="*60)

print("\\nStep 1: Preparing data with metadata...")
!python scripts/arch1_prepare_data.py

print("\\nStep 2: Generating DPR embeddings...")
!python scripts/arch1_embeddings.py

print("\\nStep 3: Building FAISS HNSW index...")
!python scripts/arch1_faiss.py

print("\\n‚úÖ System ready for queries!")

# --- CELL 12: Test RAG Pipeline ---
import sys
sys.path.insert(0, 'scripts')
from arch1_pipeline import RAGPipeline

# Initialize
pipeline = RAGPipeline(reranker_type="colbert")

# Test queries
pipeline.query("What happens in Romeo and Juliet?")
pipeline.query("What is Bitcoin?")  # Should say "Bilmiyorum"

# --- CELL 13: Interactive Mode ---
print("\\nüéÆ Interactive Mode - Type questions!")
while True:
    q = input("\\n‚ùì Question (or 'exit'): ").strip()
    if q.lower() in ['exit', 'quit', 'q']: break
    if q: pipeline.query(q)

# ============================================================
# üìã SCRIPT √áALI≈ûTIRMA SIRASI (LOCAL)
# ============================================================
#
# 1. python scripts/arch1_prepare_data.py
#    ‚Üí Veri indir, chunk'la, metadata ekle
#
# 2. python scripts/arch1_embeddings.py
#    ‚Üí DPR ile passage embedding olu≈ütur
#
# 3. python scripts/arch1_faiss.py
#    ‚Üí FAISS HNSW index kur
#
# 4. python scripts/arch1_pipeline.py -q "Soru buraya"
#    ‚Üí Tek soru sor
#
# 5. python scripts/arch1_pipeline.py -i
#    ‚Üí Interactive mod (s√ºrekli soru sor)
#
# 6. python scripts/arch1_eval.py --eval-file data/eval_qa.jsonl
#    ‚Üí T√ºm metrikleri hesapla
#
# 7. python scripts/arch1_test_incremental.py
#    ‚Üí Incremental update testi
#
# ============================================================
